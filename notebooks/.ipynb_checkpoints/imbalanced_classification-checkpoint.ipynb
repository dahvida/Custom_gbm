{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0364152",
   "metadata": {},
   "source": [
    "**Tutorial: how to use CustomGBM for imbalanced classification**  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80939b6b",
   "metadata": {},
   "source": [
    "Let's import all relevant packages and append the path to the code used for this example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "caf05e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# general packages\n",
    "import sys\n",
    "sys.path.append(\"../code\")\n",
    "from custom_gbm import CustomGBM\n",
    "from loss_function import loss_function\n",
    "from focal import Focal_loss\n",
    "import pickle as pkl\n",
    "from sklearn.metrics import *\n",
    "import time\n",
    "import lightgbm as lgb\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b1e15b",
   "metadata": {},
   "source": [
    "Let's import the data we will use for this tutorial.  \n",
    "\n",
    "We are going to predict a Quantitative Structure-Activity Relationship model for identifying CYP2C9 substrates. The raw data was downloaded from Therapeutic Data Commons (https://tdcommons.ai/single_pred_tasks/adme/#cyp2c9-substrate-carbon-mangels-et-al).  \n",
    "\n",
    "Compounds have been converted already to 208 2D molecular descriptors from RDKIT. The training set contains 468 compounds (90 are active), while the test set has 135 (38 are active). The train and test sets were obtained via scaffold split using the TCD API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a54539e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/train.pkl', 'rb') as handle:\n",
    "    train = pkl.load(handle)\n",
    "with open('../data/test.pkl', 'rb') as handle:\n",
    "    test = pkl.load(handle) \n",
    "\n",
    "x_train, y_train = train\n",
    "x_test, y_test = test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14593bb",
   "metadata": {},
   "source": [
    "Let's make a baseline using the default implementation of LightGBM using weighted cross-entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e74728f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default PR-AUC: 0.3628408002927406\n"
     ]
    }
   ],
   "source": [
    "gbm = lgb.LGBMClassifier(n_estimators=100, class_weight=\"balanced\")\n",
    "gbm.fit(x_train, y_train)\n",
    "\n",
    "predictions_default = gbm.predict_proba(x_test)[:,1]\n",
    "pr_auc_default = average_precision_score(y_test, predictions_default)\n",
    "\n",
    "print(f\"Default PR-AUC: {pr_auc_default}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1951b1d0",
   "metadata": {},
   "source": [
    "Let's make a new model using Focal loss and class weighting using CustomGBM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "23ffebe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Focal loss PR-AUC: 0.3755544120238292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dahvida/anaconda3/envs/lightning/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    }
   ],
   "source": [
    "booster_params = {\"num_boost_round\":100, \"verbose\":-100}\n",
    "loss_fn = Focal_loss\n",
    "loss_params = {\"gamma\": 2, \"class_weight\": \"balanced\"}\n",
    "gbm_1 = CustomGBM(loss_fn, loss_params, booster_params)\n",
    "\n",
    "gbm_1.fit(x_train, y_train)\n",
    "predictions_1 = gbm_1.predict(x_test)\n",
    "pr_auc_1 = average_precision_score(y_test, predictions_1)\n",
    "\n",
    "print(f\"Focal loss PR-AUC: {pr_auc_1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2082655",
   "metadata": {},
   "source": [
    "We get a nice improvement over the baseline, but we can push it even further by using the polynomial expansion of cross entropy, PolyLoss (https://arxiv.org/abs/2204.12511). This feature is already implemented either as a stand-alone loss function, or as an additional parameter for any of the custom losses. Let's try it in combination with Focal loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "65799175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Focal loss PR-AUC: 0.3908879690946251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dahvida/anaconda3/envs/lightning/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    }
   ],
   "source": [
    "booster_params = {\"num_boost_round\":100, \"verbose\":-100}\n",
    "loss_fn = Focal_loss\n",
    "loss_params = {\"gamma\": 2, \"epsilon\": 0.4}\n",
    "gbm_2 = CustomGBM(loss_fn, loss_params, booster_params)\n",
    "\n",
    "gbm_2.fit(x_train, y_train)\n",
    "predictions_2 = gbm_2.predict(x_test)\n",
    "pr_auc_2 = average_precision_score(y_test, predictions_2)\n",
    "\n",
    "print(f\"Focal+Poly loss PR-AUC: {pr_auc_2}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
