{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0789f58b",
   "metadata": {},
   "source": [
    "**Tutorial: how to wrap a custom loss function with CustomGBM**  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde7a37f",
   "metadata": {},
   "source": [
    "Let's import all relevant packages and append the path to the code used for this example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb6e95a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# general packages\n",
    "import sys\n",
    "sys.path.append(\"../code\")\n",
    "from custom_gbm import CustomGBM\n",
    "from loss_function import loss_function\n",
    "import pickle as pkl\n",
    "from sklearn.metrics import *\n",
    "import time\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "\n",
    "# jax dependencies\n",
    "from jax import config\n",
    "config.update(\"jax_enable_x64\", True)\n",
    "import jax.numpy as jnp\n",
    "from jax import jit\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad279dd3",
   "metadata": {},
   "source": [
    "Let's import the data we will use for this tutorial.  \n",
    "\n",
    "We are going to train a Quantitative Structure-Activity Relationship model for identifying CYP2C9 substrates. The raw data was downloaded from Therapeutic Data Commons (https://tdcommons.ai/single_pred_tasks/adme/#cyp2c9-substrate-carbon-mangels-et-al).  \n",
    "\n",
    "Compounds have been converted already to 208 2D molecular descriptors from RDKIT. The training set contains 468 compounds (90 are active), while the test set has 135 (38 are active). The train and test sets were obtained via scaffold split using the TCD API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e2b7efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/train.pkl', 'rb') as handle:\n",
    "    train = pkl.load(handle)\n",
    "with open('../data/test.pkl', 'rb') as handle:\n",
    "    test = pkl.load(handle) \n",
    "\n",
    "x_train, y_train = train\n",
    "x_test, y_test = test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efab7284",
   "metadata": {},
   "source": [
    "Let's make a new custom loss function. For this example, we will use the binary cross-entropy.  \n",
    "\n",
    "There are four rules to follow to successfully implement a custom loss function with CustomGBM:  \n",
    "1. The new loss function must be subclassed from the \"loss_function\" class, and the \"init\" method of the parent class must be called during the \"init\" method of the new loss function.\n",
    "2. The new loss function can have any number of arguments, but it must include the y_true vector of the training set. This then needs to be passed to the \"init\" method of the parent class.  \n",
    "3. It must include a \"call\" method, which computes the loss from the raw LightGBM output (e.g. logits).  \n",
    "4. The \"call\" method must use JAX for speeding up computation. As such, replace numpy with jnp and make sure to wrap it with the decorator shown below.  \n",
    "\n",
    "For additional examples, take a look at `focal.py` and other pre-implemented loss functions in \"../code\".\n",
    "\n",
    "The parent class takes care of computing the gradients, hessians, optimal init_score and class weights. Check `loss_function.py` in \"../code\" for further information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f07c6896",
   "metadata": {},
   "outputs": [],
   "source": [
    "class cross_entropy(loss_function):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 y_true\n",
    "                 ): \n",
    "        \n",
    "        super(cross_entropy, self).__init__(y_true)\n",
    "    \n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def __call__(self, y_true, y_pred):\n",
    "\n",
    "        p = 1/(1+jnp.exp(-y_pred))\n",
    "        q = 1-p\n",
    "        pos_loss = jnp.log(p)\n",
    "        neg_loss = jnp.log(q)\n",
    "        \n",
    "        return y_true * pos_loss + (1 - y_true) * neg_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc9408d",
   "metadata": {},
   "source": [
    "Now that we have implemented the new custom class, we can make a new LightGBM model with it using the CustomGBM API. We need to pass the class as the first argument, followed by the additional arguments of the loss (except y_true) and the arguments for the LightGBM model.  \n",
    "\n",
    "We then train the model, measure the training time and evaluate PR-AUC on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23d274c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomGBM PR-AUC: 0.35089992803682485\n",
      "CustomGBM training time (s): 0.18747258186340332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dahvida/anaconda3/envs/lightning/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    }
   ],
   "source": [
    "booster_params = {\"num_boost_round\":100, \"verbose\":-100}\n",
    "loss_fn = cross_entropy\n",
    "loss_params = {}\n",
    "gbm = CustomGBM(loss_fn, loss_params, booster_params)\n",
    "\n",
    "t_start = time.time()\n",
    "gbm.fit(x_train, y_train)\n",
    "t_end = time.time()\n",
    "t_custom = t_end - t_start\n",
    "\n",
    "predictions_custom = gbm.predict(x_test)\n",
    "pr_auc_custom = average_precision_score(y_test, predictions_custom)\n",
    "\n",
    "print(f\"CustomGBM PR-AUC: {pr_auc_custom}\")\n",
    "print(f\"CustomGBM training time (s): {t_custom}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443bd1dd",
   "metadata": {},
   "source": [
    "To ensure that the implementation is correct, let's replicate the training procedure using a standard LightGBM classifier with the default cross entropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dae03eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default PR-AUC: 0.35089992803682485\n",
      "Default training time (s): 0.1328582763671875\n"
     ]
    }
   ],
   "source": [
    "gbm = lgb.LGBMClassifier(n_estimators=100)\n",
    "\n",
    "t_start = time.time()\n",
    "gbm.fit(x_train, y_train)\n",
    "t_end = time.time()\n",
    "t_default = t_end - t_start\n",
    "\n",
    "predictions_default = gbm.predict_proba(x_test)[:,1]\n",
    "pr_auc_default = average_precision_score(y_test, predictions_default)\n",
    "\n",
    "print(f\"Default PR-AUC: {pr_auc_default}\")\n",
    "print(f\"Default training time (s): {t_default}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd4f984",
   "metadata": {},
   "source": [
    "The performance is exactly identical and the training time is in the same order of magnitude, albeit a bit slower. This is expected given that LightGBM uses C for the calculations and has analytical formulas for the gradients and the hessians, while we use numerical approximations.  \n",
    "\n",
    "As a last check, let's verify that the numerical difference between the predictions is sufficiently small:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69527a8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average prediction delta: 1.2600912305828415e-08)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Average prediction delta: {np.mean(np.abs(predictions_custom - predictions_default))})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
